import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn import neighbors
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
digit=load_digits()
n=np.shape(digit)
print('data shape',n)
print('digit dataset-keys{}'.format(digit.keys()))
print(' digit dataset-target_names{}'.format(digit['target_names']))
#there is no feature naems available
#print('digit dataset-feature names{}'.format(digit['feature_names']))
print('digit dataset-datas{}'.format(digit['data'][:1]))

#target values
print('iris dataset-target{}'.format(digit['target']))
X=digit.data
y=digit.target
print('shape of data set',np.shape(X))
print('shape of target set',np.shape(y))

fig, axes = plt.subplots(10, 10, figsize=(8, 8),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i, ax in enumerate(axes.flat):
    ax.imshow(digit.images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(digit.target[i]),
    transform=ax.transAxes, color='green')
#plt.show()
#using data feature reduction techniques to reduce the feature count
from sklearn.manifold import Isomap
iso = Isomap(n_components=2)
iso.fit(digit.data)
data_projected = iso.transform(digit.data)
print('shape of featured data{}'.format(np.shape(data_projected)))
#uing PCA to feature deruction
model=PCA(n_components=2)
model.fit(digit.data)
model_tran=model.transform(digit.data)
print('shape of PCA transformed dada',np.shape(model_tran))
Xtrain,Xtest,ytrain,ytest=train_test_split(X,y)
#train the data using GaussionNB
model_gausssion = GaussianNB()
model_gausssion.fit(Xtrain, ytrain)
y_model = model_gausssion.predict(Xtest)

print('data predicted values acuracy score',accuracy_score(ytest,y_model))
#uisng Knearest method to solve this method
for i in range(1,11):
    model_k=KNeighborsClassifier(n_neighbors=i)
    model_k.fit(Xtrain,ytrain)
    ypredict=model_k.predict(Xtest)
    print('the accuracy score of {}neighbours{}'.format(i,accuracy_score(ytest,ypredict)))

#accuracy score of 5 neighbours

model_k=KNeighborsClassifier(n_neighbors=3)
model_k.fit(Xtrain,ytrain)
ypredict=model_k.predict(Xtest)
print('the accuracy score of 3neighbours{}'.format(accuracy_score(ytest,ypredict)))

#plot the predicted value
fig, axes = plt.subplots(10, 10, figsize=(8, 8),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i, ax in enumerate(axes.flat):
    axes.imshow(digit.images[i], cmap='binary', interpolation='nearest')
    axes.text(0.05, 0.05, str(ypredict[i]), transform=ax.transAxes,
    color='green' if (ytest[i] ==ypredict[i]) else 'red')
plt.show()
